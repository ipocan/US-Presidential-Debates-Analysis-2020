{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Topic Modeling .ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNM086Pl3p7JpSuYEeI+WaE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"edQGJi4fDnB2"},"source":["# Introduction\n","\n","The goal of the topic modelling is to identify the various topics in our text. Each document in the text can include one or multiple topics. I will apply the steps of **Latent Dirichlet Allocation (LDA)** that is a technique used for topic modeling. To implement topic modeling technique, we need to use a document-term matrix and the number of topics. \n","When the topic modeling technique is applied, we will interpret the results and see if the words in each topic make sense.\n","\n","The steps in the notebook are:\n","1. Topic Modeling for All Text\n","2. Topic Modeling for Nouns Only\n","3. Topic Modeling for Nouns and Adjectives\n","4. Identify Topics in Each Document"]},{"cell_type":"markdown","metadata":{"id":"fEEdG0_1EAPY"},"source":["# Importing Libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"17HttiWPD_cL","executionInfo":{"status":"ok","timestamp":1606970468001,"user_tz":360,"elapsed":3758,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"434ef618-ea09-4971-82ee-0ec36a44a82c"},"source":["import pandas as pd\n","import pickle\n","from gensim import matutils, models\n","import scipy.sparse\n","from nltk import word_tokenize, pos_tag\n","import nltk\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","from sklearn.feature_extraction import text\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5120ES05EMAN","executionInfo":{"status":"ok","timestamp":1606970493361,"user_tz":360,"elapsed":24418,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"b4465b11-d7f0-4701-ac15-40683f6c44cf"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"66smoRVmDw4w"},"source":["## Topic Modeling for All Text"]},{"cell_type":"markdown","metadata":{"id":"wLnEE7lZEZ2l"},"source":["### 1st Presidential Debate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"5xvQib-GDurt","executionInfo":{"status":"ok","timestamp":1606970494925,"user_tz":360,"elapsed":1520,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"e52d37a0-4953-448b-d510-400acd2189c9"},"source":["first_dtm = pd.read_pickle(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/first_dtm_stop.pkl\")\n","first_dtm = first_dtm.loc[[\"Donald Trump\",\"Joe Biden\"]]\n","first_dtm.head()"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abolish</th>\n","      <th>abraham</th>\n","      <th>absolutely</th>\n","      <th>absorb</th>\n","      <th>abuse</th>\n","      <th>academic</th>\n","      <th>accept</th>\n","      <th>accompany</th>\n","      <th>accomplish</th>\n","      <th>accord</th>\n","      <th>accountable</th>\n","      <th>acknowledge</th>\n","      <th>acre</th>\n","      <th>act</th>\n","      <th>actually</th>\n","      <th>add</th>\n","      <th>addition</th>\n","      <th>additional</th>\n","      <th>address</th>\n","      <th>administration</th>\n","      <th>admission</th>\n","      <th>admit</th>\n","      <th>advantage</th>\n","      <th>advisor</th>\n","      <th>affect</th>\n","      <th>affidavit</th>\n","      <th>afford</th>\n","      <th>affordable</th>\n","      <th>afraid</th>\n","      <th>african</th>\n","      <th>africanamerican</th>\n","      <th>africanamericans</th>\n","      <th>agency</th>\n","      <th>ago</th>\n","      <th>agree</th>\n","      <th>ahead</th>\n","      <th>air</th>\n","      <th>airport</th>\n","      <th>...</th>\n","      <th>weekend</th>\n","      <th>welcome</th>\n","      <th>west</th>\n","      <th>western</th>\n","      <th>whatsoever</th>\n","      <th>wherewithal</th>\n","      <th>whichever</th>\n","      <th>whistle</th>\n","      <th>white</th>\n","      <th>wide</th>\n","      <th>wife</th>\n","      <th>willing</th>\n","      <th>win</th>\n","      <th>wing</th>\n","      <th>winner</th>\n","      <th>wipe</th>\n","      <th>wishful</th>\n","      <th>woman</th>\n","      <th>womens</th>\n","      <th>wonder</th>\n","      <th>word</th>\n","      <th>work</th>\n","      <th>worker</th>\n","      <th>workforce</th>\n","      <th>world</th>\n","      <th>worried</th>\n","      <th>worth</th>\n","      <th>wrap</th>\n","      <th>write</th>\n","      <th>wrong</th>\n","      <th>wuhan</th>\n","      <th>xenophobic</th>\n","      <th>xi</th>\n","      <th>yapping</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>zero</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>7</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>26</td>\n","      <td>5</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>2</td>\n","      <td>17</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>9</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 1419 columns</p>\n","</div>"],"text/plain":["              ability  able  abolish  abraham  ...  yes  york  young  zero\n","Donald Trump        0     1        0        0  ...    5     4      3     0\n","Joe Biden           2    17        0        0  ...    5     0      1     1\n","\n","[2 rows x 1419 columns]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"dP2P66m1GjhD","executionInfo":{"status":"ok","timestamp":1606970496818,"user_tz":360,"elapsed":642,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"c213728a-eedf-4277-b9f7-2f378a6c8b1d"},"source":["# One of the required inputs is a term-document matrix, preparing for LDA\n","\n","first_dtm = first_dtm.transpose()\n","first_dtm.head() "],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Donald Trump</th>\n","      <th>Joe Biden</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>ability</th>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>able</th>\n","      <td>1</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>abolish</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>abraham</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>absolutely</th>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            Donald Trump  Joe Biden\n","ability                0          2\n","able                   1         17\n","abolish                0          0\n","abraham                0          0\n","absolutely             3          3"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"l0oj4dlfycJh","executionInfo":{"status":"ok","timestamp":1606970499120,"user_tz":360,"elapsed":518,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n","\n","sparse_counts = scipy.sparse.csr_matrix(first_dtm)\n","corpus1 = matutils.Sparse2Corpus(sparse_counts)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"E7uZUPSUyqPg","executionInfo":{"status":"ok","timestamp":1606970500880,"user_tz":360,"elapsed":414,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n","cv1 = pickle.load(open(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/cv1_stop.pkl\", \"rb\"))\n","id2word1 = dict((v, k) for k, v in cv1.vocabulary_.items())"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOvlJ0G0zH9m"},"source":["Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term), we need to specify two other parameters - the number of topics and the number of passes."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqSi6rcezIb3","executionInfo":{"status":"ok","timestamp":1606970511837,"user_tz":360,"elapsed":415,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"c027d7d3-3a7d-4f8a-f2a2-62258f79827e"},"source":["# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n","# we need to specify two other parameters as well - the number of topics and the number of passes\n","lda1 = models.LdaModel(corpus=corpus1, id2word=id2word1, num_topics=2, passes=10)\n","lda1.print_topics()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.001*\"people\" + 0.001*\"way\" + 0.001*\"million\" + 0.001*\"president\" + 0.001*\"make\" + 0.001*\"fact\" + 0.001*\"happen\" + 0.001*\"ballot\" + 0.001*\"year\" + 0.001*\"talk\"'),\n"," (1,\n","  '0.028*\"people\" + 0.010*\"way\" + 0.010*\"make\" + 0.009*\"million\" + 0.008*\"president\" + 0.008*\"fact\" + 0.008*\"happen\" + 0.008*\"ballot\" + 0.007*\"vote\" + 0.007*\"talk\"')]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"aRN4WQHkAfi6"},"source":["These topics aren't looking too great. We've tried modifying our parameters. Let's try modifying our terms list as well."]},{"cell_type":"markdown","metadata":{"id":"jVx6VqYdAxJl"},"source":["### 2nd Presidential Debate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"z4wzfM4vzc1V","executionInfo":{"status":"ok","timestamp":1606970514199,"user_tz":360,"elapsed":768,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"f21e5da3-2977-49bf-834e-0f811bd9a887"},"source":["second_dtm = pd.read_pickle(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/second_dtm_stop.pkl\")\n","second_dtm = second_dtm.loc[[\"Donald Trump\",\"Joe Biden\"]]\n","second_dtm.head()"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>abide</th>\n","      <th>ability</th>\n","      <th>able</th>\n","      <th>abraham</th>\n","      <th>abroad</th>\n","      <th>absolutely</th>\n","      <th>abuse</th>\n","      <th>access</th>\n","      <th>accord</th>\n","      <th>account</th>\n","      <th>accountant</th>\n","      <th>accumulate</th>\n","      <th>accurate</th>\n","      <th>accuse</th>\n","      <th>act</th>\n","      <th>action</th>\n","      <th>activity</th>\n","      <th>actually</th>\n","      <th>actuary</th>\n","      <th>addition</th>\n","      <th>address</th>\n","      <th>administration</th>\n","      <th>advance</th>\n","      <th>adversary</th>\n","      <th>advisor</th>\n","      <th>advocate</th>\n","      <th>affect</th>\n","      <th>affordable</th>\n","      <th>afghanistan</th>\n","      <th>africanamerican</th>\n","      <th>agent</th>\n","      <th>ago</th>\n","      <th>agree</th>\n","      <th>ahead</th>\n","      <th>air</th>\n","      <th>alabama</th>\n","      <th>alcohol</th>\n","      <th>allow</th>\n","      <th>ally</th>\n","      <th>amendment</th>\n","      <th>...</th>\n","      <th>whistle</th>\n","      <th>white</th>\n","      <th>wife</th>\n","      <th>willing</th>\n","      <th>wilmington</th>\n","      <th>win</th>\n","      <th>wind</th>\n","      <th>windmill</th>\n","      <th>window</th>\n","      <th>windshield</th>\n","      <th>winter</th>\n","      <th>wiper</th>\n","      <th>witch</th>\n","      <th>withhold</th>\n","      <th>woman</th>\n","      <th>wonder</th>\n","      <th>wonderful</th>\n","      <th>word</th>\n","      <th>work</th>\n","      <th>worker</th>\n","      <th>world</th>\n","      <th>worldwide</th>\n","      <th>worried</th>\n","      <th>worry</th>\n","      <th>worth</th>\n","      <th>wrap</th>\n","      <th>write</th>\n","      <th>wrong</th>\n","      <th>wuhan</th>\n","      <th>xenophobia</th>\n","      <th>xenophobic</th>\n","      <th>xi</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>yesterday</th>\n","      <th>york</th>\n","      <th>young</th>\n","      <th>zero</th>\n","      <th>zone</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>10</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>16</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 1451 columns</p>\n","</div>"],"text/plain":["              abide  ability  able  abraham  ...  york  young  zero  zone\n","Donald Trump      0        0     4        6  ...     7      3     0     4\n","Joe Biden         1        2    16        2  ...     2      2     3     2\n","\n","[2 rows x 1451 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"LkZJVqAdzc4P","executionInfo":{"status":"ok","timestamp":1606970516130,"user_tz":360,"elapsed":481,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"d6981525-23d2-4a29-bdc1-39e7c49cccf3"},"source":["# One of the required inputs is a term-document matrix, preparing for LDA\n","\n","second_dtm = second_dtm.transpose()\n","second_dtm.head() "],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Donald Trump</th>\n","      <th>Joe Biden</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>abide</th>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>ability</th>\n","      <td>0</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>able</th>\n","      <td>4</td>\n","      <td>16</td>\n","    </tr>\n","    <tr>\n","      <th>abraham</th>\n","      <td>6</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>abroad</th>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["         Donald Trump  Joe Biden\n","abide               0          1\n","ability             0          2\n","able                4         16\n","abraham             6          2\n","abroad              0          0"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"LN58OiCgzc8S","executionInfo":{"status":"ok","timestamp":1606970518517,"user_tz":360,"elapsed":475,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\n","\n","sparse_counts = scipy.sparse.csr_matrix(second_dtm)\n","corpus2 = matutils.Sparse2Corpus(sparse_counts)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"7wGQgPp9BIRi","executionInfo":{"status":"ok","timestamp":1606970520268,"user_tz":360,"elapsed":623,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\n","cv2 = pickle.load(open(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/cv2_stop.pkl\", \"rb\"))\n","id2word2 = dict((v, k) for k, v in cv2.vocabulary_.items())"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N056IKE-BNQb","executionInfo":{"status":"ok","timestamp":1606970522672,"user_tz":360,"elapsed":771,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"d99ebe79-ac46-496f-ccd1-2f22d438d60c"},"source":["# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n","# we need to specify two other parameters as well - the number of topics and the number of passes\n","lda2 = models.LdaModel(corpus=corpus2, id2word=id2word2, num_topics=2, passes=10)\n","lda2.print_topics()"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.017*\"make\" + 0.016*\"people\" + 0.010*\"fact\" + 0.009*\"president\" + 0.008*\"sure\" + 0.008*\"china\" + 0.008*\"talk\" + 0.007*\"pay\" + 0.006*\"need\" + 0.006*\"happen\"'),\n"," (1,\n","  '0.015*\"people\" + 0.012*\"want\" + 0.011*\"year\" + 0.011*\"think\" + 0.010*\"million\" + 0.010*\"joe\" + 0.009*\"money\" + 0.009*\"make\" + 0.008*\"country\" + 0.008*\"talk\"')]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"Cw4YeyP0p5b1"},"source":["## Topic Modelling for Nouns Only"]},{"cell_type":"markdown","metadata":{"id":"Wsu6rEjuqEz2"},"source":["### 1st Presidential Debate"]},{"cell_type":"code","metadata":{"id":"wOVB-vSxBV6z","executionInfo":{"status":"ok","timestamp":1606970525782,"user_tz":360,"elapsed":303,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Let's create a function to pull out nouns from a string of text\n","\n","def nouns(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n","    is_noun = lambda pos: pos[:2] == 'NN'\n","    tokenized = word_tokenize(text)\n","    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n","    return ' '.join(all_nouns)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"DHLifPl2BV-I","executionInfo":{"status":"ok","timestamp":1606970528536,"user_tz":360,"elapsed":584,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"48da51bc-0c72-4dfe-be86-5fde2e2fbc71"},"source":["# Read in the cleaned data, before the CountVectorizer step\n","\n","first_clean = pd.read_pickle(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/first_whole_corpus.pkl\")\n","first_clean = first_clean.loc[[\"Donald Trump\",\"Joe Biden\"]]\n","first_clean"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","      <th>speech_time</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>How are you doing?, Thank you very much, Chris...</td>\n","      <td>36.0</td>\n","      <td>thank much chris tell simply win election elec...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>How you doing, man?, Iâ€™m well., Well, first of...</td>\n","      <td>28.0</td>\n","      <td>man well well first thank look forward mr pres...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     transcript  ...                                         clean_text\n","Donald Trump  How are you doing?, Thank you very much, Chris...  ...  thank much chris tell simply win election elec...\n","Joe Biden     How you doing, man?, Iâ€™m well., Well, first of...  ...  man well well first thank look forward mr pres...\n","\n","[2 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"KsxE6C3FBWCt","executionInfo":{"status":"ok","timestamp":1606970530180,"user_tz":360,"elapsed":766,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"125fb338-8e9b-4d55-d2da-dd9d001fdc7e"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","first_nouns = pd.DataFrame(first_clean[\"clean_text\"].apply(nouns))\n","first_nouns"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>chris tell election election consequence house...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>man thank look president people court nominee ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     clean_text\n","Donald Trump  chris tell election election consequence house...\n","Joe Biden     man thank look president people court nominee ..."]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"ygzPGeK8BWFj","executionInfo":{"status":"ok","timestamp":1606970532063,"user_tz":360,"elapsed":577,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"448a0f6a-a5ee-4cf2-88c0-e86598b3e9e5"},"source":["# Create a new document-term matrix using only nouns\n","\n","# Re-add the additional stop words since we are recreating the document-term matrix\n","add_stop_words1 = ['like',  'know', 'just', 'get', 'would', 'well', 'people','thing',\n","                   'want','way','look','joe','president','chris','biden']\n","stop_words1 = text.ENGLISH_STOP_WORDS.union(add_stop_words1)\n","\n","# Recreate a document-term matrix with only nouns\n","cvn1 = CountVectorizer(stop_words=stop_words1)\n","first_cvn = cvn1.fit_transform(first_nouns.clean_text)\n","first_dtmn = pd.DataFrame(first_cvn.toarray(), columns=cvn1.get_feature_names())\n","first_dtmn.index = first_nouns.index\n","first_dtmn"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ability</th>\n","      <th>absorb</th>\n","      <th>accompany</th>\n","      <th>accord</th>\n","      <th>acknowledge</th>\n","      <th>act</th>\n","      <th>addition</th>\n","      <th>administration</th>\n","      <th>admit</th>\n","      <th>advantage</th>\n","      <th>afford</th>\n","      <th>air</th>\n","      <th>airport</th>\n","      <th>alcohol</th>\n","      <th>america</th>\n","      <th>americans</th>\n","      <th>analysis</th>\n","      <th>andor</th>\n","      <th>announce</th>\n","      <th>answer</th>\n","      <th>antifa</th>\n","      <th>anybody</th>\n","      <th>appeal</th>\n","      <th>apple</th>\n","      <th>approval</th>\n","      <th>area</th>\n","      <th>arm</th>\n","      <th>art</th>\n","      <th>ask</th>\n","      <th>aspect</th>\n","      <th>asset</th>\n","      <th>assistance</th>\n","      <th>automobile</th>\n","      <th>ballot</th>\n","      <th>ban</th>\n","      <th>bank</th>\n","      <th>barisma</th>\n","      <th>basket</th>\n","      <th>bastard</th>\n","      <th>beat</th>\n","      <th>...</th>\n","      <th>ventilator</th>\n","      <th>vet</th>\n","      <th>vice</th>\n","      <th>view</th>\n","      <th>violence</th>\n","      <th>violent</th>\n","      <th>virginia</th>\n","      <th>vote</th>\n","      <th>vulnerable</th>\n","      <th>wade</th>\n","      <th>wage</th>\n","      <th>wait</th>\n","      <th>wall</th>\n","      <th>war</th>\n","      <th>wastepaper</th>\n","      <th>watch</th>\n","      <th>watcher</th>\n","      <th>water</th>\n","      <th>weapon</th>\n","      <th>wear</th>\n","      <th>weather</th>\n","      <th>week</th>\n","      <th>wherewithal</th>\n","      <th>whistle</th>\n","      <th>wife</th>\n","      <th>win</th>\n","      <th>winner</th>\n","      <th>wipe</th>\n","      <th>woman</th>\n","      <th>womens</th>\n","      <th>wonder</th>\n","      <th>word</th>\n","      <th>work</th>\n","      <th>workforce</th>\n","      <th>world</th>\n","      <th>xi</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 700 columns</p>\n","</div>"],"text/plain":["              ability  absorb  accompany  accord  ...  yeah  year  yes  york\n","Donald Trump        0       0          0       2  ...     1    26    4     4\n","Joe Biden           2       1          1       2  ...     1     9    0     0\n","\n","[2 rows x 700 columns]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"am5_q7eIvqkm","executionInfo":{"status":"ok","timestamp":1606970535197,"user_tz":360,"elapsed":401,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Create the gensim corpus\n","corpusn1 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(first_dtmn.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordn1 = dict((v, k) for k, v in cvn1.vocabulary_.items())"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fvnsWyQ9vqnn","executionInfo":{"status":"ok","timestamp":1606970549171,"user_tz":360,"elapsed":465,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"01c33d5b-3b4e-4c84-c3bc-3216a69c16d5"},"source":["# Let's create 2 different topic modeling\n","ldan1 = models.LdaModel(corpus=corpusn1, num_topics=2, id2word=id2wordn1, passes=10)\n","ldan1.print_topics()"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.019*\"year\" + 0.018*\"country\" + 0.016*\"ballot\" + 0.014*\"dollar\" + 0.014*\"election\" + 0.012*\"law\" + 0.011*\"lot\" + 0.010*\"job\" + 0.010*\"place\" + 0.010*\"car\"'),\n"," (1,\n","  '0.026*\"fact\" + 0.017*\"vote\" + 0.017*\"deal\" + 0.015*\"number\" + 0.014*\"job\" + 0.011*\"talk\" + 0.011*\"ballot\" + 0.011*\"tax\" + 0.009*\"man\" + 0.008*\"election\"')]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0HSzmbBm6So5","executionInfo":{"status":"ok","timestamp":1606970588488,"user_tz":360,"elapsed":359,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"f67c844d-cd27-454a-e7d3-38e82510d43c"},"source":["corpus_transformed = ldan1[corpusn1]\n","\n","list(zip([a for [(a,b)] in corpus_transformed], first_dtmn.index))"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 'Donald Trump'), (1, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"piVDJ925wI9I"},"source":["### 2nd Presidential Debate"]},{"cell_type":"code","metadata":{"id":"fj_E3QafwMxt","executionInfo":{"status":"ok","timestamp":1606970876122,"user_tz":360,"elapsed":1758,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Let's create a function to pull out nouns from a string of text\n","\n","def nouns(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n","    is_noun = lambda pos: pos[:2] == 'NN'\n","    tokenized = word_tokenize(text)\n","    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n","    return ' '.join(all_nouns)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"yn50mH7JwSRF","executionInfo":{"status":"ok","timestamp":1606970878677,"user_tz":360,"elapsed":994,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"a29dc2ad-5ccf-4cff-ee4d-00b148c71949"},"source":["# Read in the cleaned data, before the CountVectorizer step\n","\n","second_clean = pd.read_pickle(\"/content/drive/MyDrive/Data Science/us election presidential debates/pickles/second_whole_corpus.pkl\")\n","second_clean = first_clean.loc[[\"Donald Trump\",\"Joe Biden\"]]\n","second_clean"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>transcript</th>\n","      <th>speech_time</th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>How are you doing?, Thank you very much, Chris...</td>\n","      <td>36.0</td>\n","      <td>thank much chris tell simply win election elec...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>How you doing, man?, Iâ€™m well., Well, first of...</td>\n","      <td>28.0</td>\n","      <td>man well well first thank look forward mr pres...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     transcript  ...                                         clean_text\n","Donald Trump  How are you doing?, Thank you very much, Chris...  ...  thank much chris tell simply win election elec...\n","Joe Biden     How you doing, man?, Iâ€™m well., Well, first of...  ...  man well well first thank look forward mr pres...\n","\n","[2 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"pUm2hV7LwSUk","executionInfo":{"status":"ok","timestamp":1606970881218,"user_tz":360,"elapsed":588,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"284a4f35-902e-48e7-9247-94e1369a5039"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","second_nouns = pd.DataFrame(second_clean[\"clean_text\"].apply(nouns))\n","second_nouns"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>chris tell election election consequence house...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>man thank look president people court nominee ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     clean_text\n","Donald Trump  chris tell election election consequence house...\n","Joe Biden     man thank look president people court nominee ..."]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"pzHoY6ECwSXq","executionInfo":{"status":"ok","timestamp":1606970882156,"user_tz":360,"elapsed":575,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"5c3a976c-0a0d-4cef-d642-bac721e1a1dc"},"source":["# Create a new document-term matrix using only nouns\n","\n","# Re-add the additional stop words since we are recreating the document-term matrix\n","add_stop_words2 = ['like',  'know', 'just', 'get', 'would', 'well', 'people',\n","                   'thing', 'want','look','joe','president','chris','biden','way']\n","stop_words2 = text.ENGLISH_STOP_WORDS.union(add_stop_words2)\n","\n","# Recreate a document-term matrix with only nouns\n","cvn2 = CountVectorizer(stop_words=stop_words2)\n","second_cvn = cvn2.fit_transform(second_nouns.clean_text)\n","second_dtmn = pd.DataFrame(second_cvn.toarray(), columns=cvn2.get_feature_names())\n","second_dtmn.index = second_nouns.index\n","second_dtmn"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ability</th>\n","      <th>absorb</th>\n","      <th>accompany</th>\n","      <th>accord</th>\n","      <th>acknowledge</th>\n","      <th>act</th>\n","      <th>addition</th>\n","      <th>administration</th>\n","      <th>admit</th>\n","      <th>advantage</th>\n","      <th>afford</th>\n","      <th>air</th>\n","      <th>airport</th>\n","      <th>alcohol</th>\n","      <th>america</th>\n","      <th>americans</th>\n","      <th>analysis</th>\n","      <th>andor</th>\n","      <th>announce</th>\n","      <th>answer</th>\n","      <th>antifa</th>\n","      <th>anybody</th>\n","      <th>appeal</th>\n","      <th>apple</th>\n","      <th>approval</th>\n","      <th>area</th>\n","      <th>arm</th>\n","      <th>art</th>\n","      <th>ask</th>\n","      <th>aspect</th>\n","      <th>asset</th>\n","      <th>assistance</th>\n","      <th>automobile</th>\n","      <th>ballot</th>\n","      <th>ban</th>\n","      <th>bank</th>\n","      <th>barisma</th>\n","      <th>basket</th>\n","      <th>bastard</th>\n","      <th>beat</th>\n","      <th>...</th>\n","      <th>ventilator</th>\n","      <th>vet</th>\n","      <th>vice</th>\n","      <th>view</th>\n","      <th>violence</th>\n","      <th>violent</th>\n","      <th>virginia</th>\n","      <th>vote</th>\n","      <th>vulnerable</th>\n","      <th>wade</th>\n","      <th>wage</th>\n","      <th>wait</th>\n","      <th>wall</th>\n","      <th>war</th>\n","      <th>wastepaper</th>\n","      <th>watch</th>\n","      <th>watcher</th>\n","      <th>water</th>\n","      <th>weapon</th>\n","      <th>wear</th>\n","      <th>weather</th>\n","      <th>week</th>\n","      <th>wherewithal</th>\n","      <th>whistle</th>\n","      <th>wife</th>\n","      <th>win</th>\n","      <th>winner</th>\n","      <th>wipe</th>\n","      <th>woman</th>\n","      <th>womens</th>\n","      <th>wonder</th>\n","      <th>word</th>\n","      <th>work</th>\n","      <th>workforce</th>\n","      <th>world</th>\n","      <th>xi</th>\n","      <th>yeah</th>\n","      <th>year</th>\n","      <th>yes</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>22</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>6</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>24</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 700 columns</p>\n","</div>"],"text/plain":["              ability  absorb  accompany  accord  ...  yeah  year  yes  york\n","Donald Trump        0       0          0       2  ...     1    26    4     4\n","Joe Biden           2       1          1       2  ...     1     9    0     0\n","\n","[2 rows x 700 columns]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"ix31aNwowSc2","executionInfo":{"status":"ok","timestamp":1606970885418,"user_tz":360,"elapsed":397,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Create the gensim corpus\n","corpusn2 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(second_dtmn.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordn2 = dict((v, k) for k, v in cvn2.vocabulary_.items())"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S2jgziW5wSgT","executionInfo":{"status":"ok","timestamp":1606970886350,"user_tz":360,"elapsed":400,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"b3b945da-32e5-45ed-a6c9-a9f8ffff0e2a"},"source":["# Let's create 2 different topic modeling\n","ldan2 = models.LdaModel(corpus=corpusn2, num_topics=2, id2word=id2wordn2, passes=10)\n","ldan2.print_topics()"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.027*\"fact\" + 0.017*\"vote\" + 0.017*\"deal\" + 0.016*\"number\" + 0.014*\"job\" + 0.011*\"talk\" + 0.011*\"ballot\" + 0.011*\"tax\" + 0.009*\"man\" + 0.008*\"election\"'),\n"," (1,\n","  '0.019*\"year\" + 0.018*\"country\" + 0.016*\"ballot\" + 0.014*\"election\" + 0.014*\"dollar\" + 0.012*\"law\" + 0.011*\"lot\" + 0.010*\"job\" + 0.010*\"place\" + 0.010*\"car\"')]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K1leFjx-7dhv","executionInfo":{"status":"ok","timestamp":1606970967509,"user_tz":360,"elapsed":526,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"87be3e33-5e78-4cfa-8d86-4386dec5d4ec"},"source":["corpus_transformed = ldan2[corpusn2]\n","\n","list(zip([a for [(a,b)] in corpus_transformed], second_dtmn.index))"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 'Donald Trump'), (0, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"49RWuzbJ48Ws"},"source":["## Topic Modeling for Nouns and Adjectives"]},{"cell_type":"markdown","metadata":{"id":"vGLmVnUB5JlI"},"source":["### 1st Presidential Debate"]},{"cell_type":"code","metadata":{"id":"tXcfrPsM4J7T","executionInfo":{"status":"ok","timestamp":1606970933358,"user_tz":360,"elapsed":481,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Let's create a function to pull out nouns from a string of text\n","def nouns_adj(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n","    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n","    tokenized = word_tokenize(text)\n","    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n","    return ' '.join(nouns_adj)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"0i8URnJD4J-5","executionInfo":{"status":"ok","timestamp":1606970935606,"user_tz":360,"elapsed":674,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"aa7d8ca4-363a-4c84-f740-31a9852a79be"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","first_nouns_adj = pd.DataFrame(first_clean[\"clean_text\"].apply(nouns_adj))\n","first_nouns_adj"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>much chris tell win election election conseque...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>man first thank look mr president american peo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     clean_text\n","Donald Trump  much chris tell win election election conseque...\n","Joe Biden     man first thank look mr president american peo..."]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"odRIPGP64KEF","executionInfo":{"status":"ok","timestamp":1606970939084,"user_tz":360,"elapsed":672,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"4c94d09b-94d2-4713-93d3-319e560b3824"},"source":["# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n","cvna1 = CountVectorizer(stop_words=stop_words1, max_df=.8)\n","first_cvna = cvna1.fit_transform(first_nouns_adj[\"clean_text\"])\n","first_dtmna = pd.DataFrame(first_cvna.toarray(), columns=cvna1.get_feature_names())\n","first_dtmna.index = first_nouns_adj.index\n","first_dtmna"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ability</th>\n","      <th>absorb</th>\n","      <th>academic</th>\n","      <th>accompany</th>\n","      <th>accomplish</th>\n","      <th>accountable</th>\n","      <th>acknowledge</th>\n","      <th>acre</th>\n","      <th>additional</th>\n","      <th>admit</th>\n","      <th>advantage</th>\n","      <th>afford</th>\n","      <th>affordable</th>\n","      <th>african</th>\n","      <th>agree</th>\n","      <th>air</th>\n","      <th>airport</th>\n","      <th>alcohol</th>\n","      <th>america</th>\n","      <th>american</th>\n","      <th>americans</th>\n","      <th>analysis</th>\n","      <th>andor</th>\n","      <th>announce</th>\n","      <th>antisemitic</th>\n","      <th>appeal</th>\n","      <th>apple</th>\n","      <th>appropriate</th>\n","      <th>approval</th>\n","      <th>approve</th>\n","      <th>area</th>\n","      <th>arm</th>\n","      <th>art</th>\n","      <th>aspect</th>\n","      <th>asset</th>\n","      <th>assistance</th>\n","      <th>automobile</th>\n","      <th>aware</th>\n","      <th>ban</th>\n","      <th>barisma</th>\n","      <th>...</th>\n","      <th>vet</th>\n","      <th>violence</th>\n","      <th>virginia</th>\n","      <th>vulnerable</th>\n","      <th>wade</th>\n","      <th>wage</th>\n","      <th>wall</th>\n","      <th>war</th>\n","      <th>warm</th>\n","      <th>wash</th>\n","      <th>wastepaper</th>\n","      <th>watch</th>\n","      <th>watcher</th>\n","      <th>water</th>\n","      <th>wealthy</th>\n","      <th>weapon</th>\n","      <th>weather</th>\n","      <th>week</th>\n","      <th>west</th>\n","      <th>wherewithal</th>\n","      <th>whistle</th>\n","      <th>wide</th>\n","      <th>wife</th>\n","      <th>willing</th>\n","      <th>winner</th>\n","      <th>wipe</th>\n","      <th>wishful</th>\n","      <th>woman</th>\n","      <th>womens</th>\n","      <th>wonder</th>\n","      <th>workforce</th>\n","      <th>worried</th>\n","      <th>worth</th>\n","      <th>wrap</th>\n","      <th>write</th>\n","      <th>wuhan</th>\n","      <th>xenophobic</th>\n","      <th>xi</th>\n","      <th>yes</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 698 columns</p>\n","</div>"],"text/plain":["              ability  absorb  academic  accompany  ...  xenophobic  xi  yes  york\n","Donald Trump        0       0         1          0  ...           1   0    4     4\n","Joe Biden           2       1         0          1  ...           0   2    0     0\n","\n","[2 rows x 698 columns]"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"eAIW4bZ14KCP","executionInfo":{"status":"ok","timestamp":1606970944753,"user_tz":360,"elapsed":326,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Create the gensim corpus\n","corpusna1 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(first_dtmna.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordna1 = dict((v, k) for k, v in cvna1.vocabulary_.items())"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BesjRGb56AGP","executionInfo":{"status":"ok","timestamp":1606971091454,"user_tz":360,"elapsed":424,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"ee16085b-129e-4ec8-f5d4-cfb008408953"},"source":["# Let's create 2 different topic modeling\n","ldana1 = models.LdaModel(corpus=corpusna1, num_topics=2, id2word=id2wordna1, passes=10)\n","ldana1.print_topics()"],"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.016*\"place\" + 0.010*\"month\" + 0.008*\"city\" + 0.008*\"okay\" + 0.008*\"judge\" + 0.008*\"close\" + 0.007*\"period\" + 0.007*\"november\" + 0.007*\"management\" + 0.006*\"excuse\"'),\n"," (1,\n","  '0.015*\"american\" + 0.008*\"united\" + 0.007*\"create\" + 0.007*\"discredit\" + 0.006*\"home\" + 0.006*\"affordable\" + 0.006*\"violence\" + 0.006*\"director\" + 0.005*\"blow\" + 0.005*\"gas\"')]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLFFpEW35kUx","executionInfo":{"status":"ok","timestamp":1606971040400,"user_tz":360,"elapsed":634,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"110848fb-5755-4cdb-f4dc-9e6322c7f141"},"source":["corpus_transformed = ldana1[corpusna1]\n","list(zip([a for [(a,b)] in corpus_transformed], first_dtmna.index))"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 'Donald Trump'), (1, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"markdown","metadata":{"id":"sF1eNXQ585X-"},"source":["### 2nd Presidential Debate"]},{"cell_type":"code","metadata":{"id":"oTXRKY896fue","executionInfo":{"status":"ok","timestamp":1606971101260,"user_tz":360,"elapsed":349,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Let's create a function to pull out nouns from a string of text\n","def nouns_adj(text):\n","    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n","    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n","    tokenized = word_tokenize(text)\n","    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n","    return ' '.join(nouns_adj)"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":110},"id":"rwN6rb1F9Uu1","executionInfo":{"status":"ok","timestamp":1606971102319,"user_tz":360,"elapsed":576,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"d40affd7-b32a-4ac1-e1b9-23fa4ff6de57"},"source":["# Apply the nouns function to the transcripts to filter only on nouns\n","second_nouns_adj = pd.DataFrame(second_clean[\"clean_text\"].apply(nouns_adj))\n","second_nouns_adj"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>clean_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>much chris tell win election election conseque...</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>man first thank look mr president american peo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                     clean_text\n","Donald Trump  much chris tell win election election conseque...\n","Joe Biden     man first thank look mr president american peo..."]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":194},"id":"IVlZ5HeC9VQn","executionInfo":{"status":"ok","timestamp":1606971103481,"user_tz":360,"elapsed":445,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"c0954d11-033f-41b3-e190-983f6cade107"},"source":["# Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n","cvna2 = CountVectorizer(stop_words=stop_words2, max_df=.8)\n","second_cvna = cvna2.fit_transform(second_nouns_adj[\"clean_text\"])\n","second_dtmna = pd.DataFrame(second_cvna.toarray(), columns=cvna2.get_feature_names())\n","second_dtmna.index = second_nouns_adj.index\n","second_dtmna"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ability</th>\n","      <th>absorb</th>\n","      <th>academic</th>\n","      <th>accompany</th>\n","      <th>accomplish</th>\n","      <th>accountable</th>\n","      <th>acknowledge</th>\n","      <th>acre</th>\n","      <th>additional</th>\n","      <th>admit</th>\n","      <th>advantage</th>\n","      <th>afford</th>\n","      <th>affordable</th>\n","      <th>african</th>\n","      <th>agree</th>\n","      <th>air</th>\n","      <th>airport</th>\n","      <th>alcohol</th>\n","      <th>america</th>\n","      <th>american</th>\n","      <th>americans</th>\n","      <th>analysis</th>\n","      <th>andor</th>\n","      <th>announce</th>\n","      <th>antisemitic</th>\n","      <th>appeal</th>\n","      <th>apple</th>\n","      <th>appropriate</th>\n","      <th>approval</th>\n","      <th>approve</th>\n","      <th>area</th>\n","      <th>arm</th>\n","      <th>art</th>\n","      <th>aspect</th>\n","      <th>asset</th>\n","      <th>assistance</th>\n","      <th>automobile</th>\n","      <th>aware</th>\n","      <th>ban</th>\n","      <th>barisma</th>\n","      <th>...</th>\n","      <th>vet</th>\n","      <th>violence</th>\n","      <th>virginia</th>\n","      <th>vulnerable</th>\n","      <th>wade</th>\n","      <th>wage</th>\n","      <th>wall</th>\n","      <th>war</th>\n","      <th>warm</th>\n","      <th>wash</th>\n","      <th>wastepaper</th>\n","      <th>watch</th>\n","      <th>watcher</th>\n","      <th>water</th>\n","      <th>wealthy</th>\n","      <th>weapon</th>\n","      <th>weather</th>\n","      <th>week</th>\n","      <th>west</th>\n","      <th>wherewithal</th>\n","      <th>whistle</th>\n","      <th>wide</th>\n","      <th>wife</th>\n","      <th>willing</th>\n","      <th>winner</th>\n","      <th>wipe</th>\n","      <th>wishful</th>\n","      <th>woman</th>\n","      <th>womens</th>\n","      <th>wonder</th>\n","      <th>workforce</th>\n","      <th>worried</th>\n","      <th>worth</th>\n","      <th>wrap</th>\n","      <th>write</th>\n","      <th>wuhan</th>\n","      <th>xenophobic</th>\n","      <th>xi</th>\n","      <th>yes</th>\n","      <th>york</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Donald Trump</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>Joe Biden</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","      <td>14</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>5</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2 rows Ã— 698 columns</p>\n","</div>"],"text/plain":["              ability  absorb  academic  accompany  ...  xenophobic  xi  yes  york\n","Donald Trump        0       0         1          0  ...           1   0    4     4\n","Joe Biden           2       1         0          1  ...           0   2    0     0\n","\n","[2 rows x 698 columns]"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"id":"rGuCwWaA9VTx","executionInfo":{"status":"ok","timestamp":1606971109568,"user_tz":360,"elapsed":408,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}}},"source":["# Create the gensim corpus\n","corpusna2 = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(second_dtmna.transpose()))\n","\n","# Create the vocabulary dictionary\n","id2wordna2 = dict((v, k) for k, v in cvna2.vocabulary_.items())"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bi54TABL9VZG","executionInfo":{"status":"ok","timestamp":1606971176485,"user_tz":360,"elapsed":478,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"28e9d130-1510-4212-8020-211a8320a625"},"source":["# Let's create 2 different topic modeling\n","ldana2 = models.LdaModel(corpus=corpusna2, num_topics=2, id2word=id2wordna2, passes=10)\n","ldana2.print_topics()"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.016*\"place\" + 0.010*\"month\" + 0.008*\"close\" + 0.008*\"city\" + 0.008*\"okay\" + 0.008*\"judge\" + 0.007*\"management\" + 0.007*\"november\" + 0.007*\"period\" + 0.006*\"super\"'),\n"," (1,\n","  '0.015*\"american\" + 0.008*\"united\" + 0.007*\"create\" + 0.007*\"discredit\" + 0.006*\"home\" + 0.006*\"director\" + 0.006*\"violence\" + 0.006*\"affordable\" + 0.005*\"panic\" + 0.005*\"proposal\"')]"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5FIxyr8z5ehk","executionInfo":{"status":"ok","timestamp":1606971128941,"user_tz":360,"elapsed":580,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"518d0bac-5695-4fed-87c7-01bae937ef0f"},"source":["corpus_transformed = ldana2[corpusna2]\n","list(zip([a for [(a,b)] in corpus_transformed], second_dtmna.index))"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 'Donald Trump'), (1, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"iM0l08ar-YXO"},"source":["## Identify Topics in Each Document\n","When we look at the topic modelings, the nouns modeling made the most sense. So let's pull that down here and run it through some more iterations to get more fine-tuned topics."]},{"cell_type":"markdown","metadata":{"id":"qWJqT33M-mqB"},"source":["### 1st Presidential Debate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FCHQAWPc9VXb","executionInfo":{"status":"ok","timestamp":1606971257142,"user_tz":360,"elapsed":1281,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"27e934b7-3bc9-4dc9-a5f2-314bd74ff99c"},"source":["# Our final LDA model \n","ldan1 = models.LdaModel(corpus=corpusn1, num_topics=2, id2word=id2wordn1, passes=10)\n","ldan1.print_topics()"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.026*\"fact\" + 0.017*\"deal\" + 0.017*\"vote\" + 0.015*\"number\" + 0.014*\"job\" + 0.011*\"talk\" + 0.011*\"ballot\" + 0.011*\"tax\" + 0.009*\"man\" + 0.008*\"care\"'),\n"," (1,\n","  '0.019*\"year\" + 0.018*\"country\" + 0.016*\"ballot\" + 0.014*\"election\" + 0.014*\"dollar\" + 0.012*\"law\" + 0.011*\"lot\" + 0.010*\"job\" + 0.010*\"place\" + 0.009*\"car\"')]"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"markdown","metadata":{"id":"Z0BZlsi--2ZV"},"source":["These topics look pretty decent. Let's settle on these for now.\n","* Topic 0: election, vote, job, tax \n","* Topic 1: country, job, economic affairs, election\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5HOkQlTJ-1XD","executionInfo":{"status":"ok","timestamp":1606971232276,"user_tz":360,"elapsed":421,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"8a0d808a-8f75-472a-ce60-b3b69c780e07"},"source":["# Let's take a look at which topics each transcript contains\n","corpus_transformed = ldan1[corpusn1]\n","\n","list(zip([a for [(a,b)] in corpus_transformed], first_dtmn.index))"],"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(1, 'Donald Trump'), (0, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"E7RSlL9T_LQ4"},"source":["For now, our topic modelling makes sense and these are our topics for each politician.\n","\n","* Topic 0: election, vote, job, tax **[Joe Biden]** \n","* Topic 1: country, job, economic affairs, election **[Donald Trump]**\n"]},{"cell_type":"markdown","metadata":{"id":"Lyv10paMaq3I"},"source":["* **Finding**            \n","There were specific discussion topic in the debate, similar to the topics we found in our model. What we notice here is the focus point of each politician during the debate."]},{"cell_type":"markdown","metadata":{"id":"cqAOnW-M_Nro"},"source":["### 2nd Presidential Debate"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AxtVj73S_MbJ","executionInfo":{"status":"ok","timestamp":1606971374610,"user_tz":360,"elapsed":418,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"15512297-9ab1-4ee6-cb09-af20322df393"},"source":["# Our final LDA model (for now)\n","ldan2 = models.LdaModel(corpus=corpusn2, num_topics=2, id2word=id2wordn2, passes=10)\n","ldan2.print_topics()"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0,\n","  '0.019*\"year\" + 0.018*\"country\" + 0.016*\"ballot\" + 0.014*\"election\" + 0.014*\"dollar\" + 0.012*\"law\" + 0.011*\"lot\" + 0.010*\"job\" + 0.010*\"place\" + 0.010*\"car\"'),\n"," (1,\n","  '0.026*\"fact\" + 0.017*\"vote\" + 0.017*\"deal\" + 0.015*\"number\" + 0.014*\"job\" + 0.011*\"talk\" + 0.011*\"ballot\" + 0.011*\"tax\" + 0.009*\"man\" + 0.008*\"election\"')]"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"markdown","metadata":{"id":"_9m-pBg6F5sN"},"source":["These topics look pretty decent. Let's settle on these for now.\n","* Topic 0: country, economic affairs, law, election\n","* Topic 1: election, vote, job, tax,"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x2sBE_k7_VLy","executionInfo":{"status":"ok","timestamp":1606971380237,"user_tz":360,"elapsed":631,"user":{"displayName":"Isa Pocan","photoUrl":"","userId":"13107079418452594453"}},"outputId":"27627b48-d6f6-4ac9-caac-7b21c4961e3c"},"source":["# Let's take a look at which topics each transcript contains\n","corpus_transformed = ldan2[corpusn2]\n","list(zip([a for [(a,b)] in corpus_transformed], second_dtmn.index))"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(0, 'Donald Trump'), (1, 'Joe Biden')]"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"g4j5Cd7aGMgZ"},"source":["For now, our topic modelling makes sense and these are our topics for each politician.\n","* Topic 0: country, economic affairs, law, election **[Donald Trump]**\n","* Topic 1: election, vote, job, tax **[Joe Biden]**"]},{"cell_type":"markdown","metadata":{"id":"82vTzqglb4U_"},"source":["* **Finding**              \n","No surprise, we have similar focused topics in second debate as well."]}]}