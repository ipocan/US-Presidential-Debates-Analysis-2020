{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'importlib_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catalogue.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 3.8\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'importlib.metadata'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-145-2236345af5cd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0municodedata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoktok\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mToktokTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# These are imported as part of the API\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mthinc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mneural\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprefer_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequire_gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mabout\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m__version__\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_registry\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\thinc\\_registry.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mregistry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0moptimizers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcatalogue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"thinc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"optimizers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\catalogue.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mimport\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'importlib_metadata'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "\n",
    "nlp = spacy.load('en', parse=True, tag=True, entity=True)\n",
    "tokenizer = ToktokTokenizer()\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "pd.set_option('max_colwidth',100)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_debate = pd.read_csv(\"datasets/us_election_2020_1st_presidential_debate.csv\")\n",
    "second_debate = pd.read_csv(\"datasets/us_election_2020_2nd_presidential_debate.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>minute</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chris Wallace</td>\n",
       "      <td>01:20</td>\n",
       "      <td>Good evening from the Health Education Campus of Case Western Reserve University and the Clevela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chris Wallace</td>\n",
       "      <td>02:10</td>\n",
       "      <td>This debate is being conducted under health and safety protocols designed by the Cleveland Clini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>02:49</td>\n",
       "      <td>How you doing, man?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>President Donald J. Trump</td>\n",
       "      <td>02:51</td>\n",
       "      <td>How are you doing?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Vice President Joe Biden</td>\n",
       "      <td>02:51</td>\n",
       "      <td>I’m well.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     speaker minute  \\\n",
       "0              Chris Wallace  01:20   \n",
       "1              Chris Wallace  02:10   \n",
       "2   Vice President Joe Biden  02:49   \n",
       "3  President Donald J. Trump  02:51   \n",
       "4   Vice President Joe Biden  02:51   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Good evening from the Health Education Campus of Case Western Reserve University and the Clevela...  \n",
       "1  This debate is being conducted under health and safety protocols designed by the Cleveland Clini...  \n",
       "2                                                                                  How you doing, man?  \n",
       "3                                                                                   How are you doing?  \n",
       "4                                                                                            I’m well.  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_debate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 789 entries, 0 to 788\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   speaker  789 non-null    object\n",
      " 1   minute   788 non-null    object\n",
      " 2   text     789 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 18.6+ KB\n"
     ]
    }
   ],
   "source": [
    "first_debate.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>minute</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>00:18</td>\n",
       "      <td>Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>07:37</td>\n",
       "      <td>How are you doing? How are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>07:58</td>\n",
       "      <td>And I do want to say a very good evening to both of you. This debate will cover six major topics...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>08:27</td>\n",
       "      <td>The goal is for you to hear each other and for the American people to hear every word of what yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kristen Welker</td>\n",
       "      <td>09:03</td>\n",
       "      <td>… during this next stage of the coronavirus crisis. Two minutes uninterrupted.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          speaker minute  \\\n",
       "0  Kristen Welker  00:18   \n",
       "1    Donald Trump  07:37   \n",
       "2  Kristen Welker  07:58   \n",
       "3  Kristen Welker  08:27   \n",
       "4  Kristen Welker  09:03   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for ...  \n",
       "1                                                                      How are you doing? How are you?  \n",
       "2  And I do want to say a very good evening to both of you. This debate will cover six major topics...  \n",
       "3  The goal is for you to hear each other and for the American people to hear every word of what yo...  \n",
       "4                       … during this next stage of the coronavirus crisis. Two minutes uninterrupted.  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_debate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 512 entries, 0 to 511\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   speaker  512 non-null    object\n",
      " 1   minute   512 non-null    object\n",
      " 2   text     512 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 12.1+ KB\n"
     ]
    }
   ],
   "source": [
    "second_debate.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Chris Wallace', 'Vice President Joe Biden',\n",
       "       'President Donald J. Trump', 'Chris Wallace:'], dtype=object)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check unique speakers in first debate\n",
    "first_debate[\"speaker\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see in our unique speakers, we have 2 moderators in our dataframe. They are same person but written uncorrect therefore I will rename moderator name by correcting mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_debate[\"speaker\"] = first_debate[\"speaker\"].replace({\"Chris Wallace:\": \"Chris Wallace\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Donald Trump</th>\n",
       "      <td>How are you doing?, Thank you very much, Chris. I will tell you very simply. We won the election...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Chris Wallace</th>\n",
       "      <td>Good evening from the Health Education Campus of Case Western Reserve University and the Clevela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joe Biden</th>\n",
       "      <td>How you doing, man?, I’m well., Well, first of all, thank you for doing this and looking forward...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                        transcript\n",
       "Donald Trump   How are you doing?, Thank you very much, Chris. I will tell you very simply. We won the election...\n",
       "Chris Wallace  Good evening from the Health Education Campus of Case Western Reserve University and the Clevela...\n",
       "Joe Biden      How you doing, man?, I’m well., Well, first of all, thank you for doing this and looking forward..."
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create corpus by combining text in all rows and making one text row for each speaker.\n",
    "dict_dt={'transcript':', '.join(first_debate[first_debate[\"speaker\"]==\"President Donald J. Trump\"][\"text\"])}\n",
    "dt_df = pd.DataFrame(data=dict_dt, index=[\"Donald Trump\"])\n",
    "\n",
    "dict_cw={'transcript':', '.join(first_debate[first_debate[\"speaker\"]==\"Chris Wallace\"][\"text\"])}\n",
    "cw_df = pd.DataFrame(data=dict_cw, index=[\"Chris Wallace\"])\n",
    "\n",
    "dict_jb={'transcript':', '.join(first_debate[first_debate[\"speaker\"]==\"Vice President Joe Biden\"][\"text\"])}\n",
    "jb_df = pd.DataFrame(data=dict_jb, index=[\"Joe Biden\"])\n",
    "\n",
    "first_data = pd.concat([dt_df, cw_df,jb_df])\n",
    "first_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can either create different dataframes for each speaker.\n",
    "first_CW = first_debate[first_debate[\"speaker\"]==\"Chris Wallace\"]\n",
    "first_DT = first_debate[first_debate[\"speaker\"]==\"President Donald J. Trump\"]\n",
    "first_JB = first_debate[first_debate[\"speaker\"]==\"Vice President Joe Biden\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Kristen Welker', 'Donald Trump', 'Joe Biden'], dtype=object)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check unique speakers in second debate\n",
    "second_debate[\"speaker\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>transcript</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Donald Trump</th>\n",
       "      <td>How are you doing? How are you?, So as you know, 2.2 million people modeled out, were expected t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Kristen Welker</th>\n",
       "      <td>Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Joe Biden</th>\n",
       "      <td>220,000 Americans dead. You hear nothing else I say tonight, hear this. Anyone who is responsibl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         transcript\n",
       "Donald Trump    How are you doing? How are you?, So as you know, 2.2 million people modeled out, were expected t...\n",
       "Kristen Welker  Good evening, everyone. Good evening. Thank you so much for being here. It is such an honor for ...\n",
       "Joe Biden       220,000 Americans dead. You hear nothing else I say tonight, hear this. Anyone who is responsibl..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We are going to create corpus by combining text in all rows and making one text row for each speaker.\n",
    "dt_dict={'transcript':', '.join(second_debate[second_debate[\"speaker\"]==\"Donald Trump\"][\"text\"])}\n",
    "df_dt = pd.DataFrame(data=dt_dict, index=[\"Donald Trump\"])\n",
    "\n",
    "kw_dict={'transcript':', '.join(second_debate[second_debate[\"speaker\"]==\"Kristen Welker\"][\"text\"])}\n",
    "df_kw = pd.DataFrame(data=kw_dict, index=[\"Kristen Welker\"])\n",
    "\n",
    "jb_dict={'transcript':', '.join(second_debate[second_debate[\"speaker\"]==\"Joe Biden\"][\"text\"])}\n",
    "df_jb = pd.DataFrame(data=jb_dict, index=[\"Joe Biden\"])\n",
    "\n",
    "second_data = pd.concat([df_dt, df_kw,df_jb])\n",
    "second_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Expanding Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \n",
    "    '''Make text lowercase.'''\n",
    "    text = text.lower()\n",
    "    \n",
    "    ''' Expanding contractions'''\n",
    "    text = re.sub(\"that’s\",\"that is\",text)\n",
    "    text = re.sub(\"there’s\",\"there is\",text)\n",
    "    text = re.sub(\"here’s\",\"here is\",text)\n",
    "    text = re.sub(\"what’s\",\"what is\",text)\n",
    "    text = re.sub(\"where’s\",\"where is\",text)\n",
    "    text = re.sub(\"who’s\",\"who is\",text)\n",
    "    text = re.sub(\"i’m\",\"i am\",text)\n",
    "    text = re.sub(\"it’s\",\"it is\",text)\n",
    "    text = re.sub(\"she’s\",\"she is\",text)\n",
    "    text = re.sub(\"he’s\",\"he is\",text)\n",
    "    text = re.sub(\"they’re\",\"they are\",text)\n",
    "    text = re.sub(\"we’re\",\"we are\",text)\n",
    "    text = re.sub(\"you’re\",\"you are\",text)\n",
    "    text = re.sub(\"who’re\",\"who are\",text)\n",
    "    text = re.sub(\"i’ll\",\"i will\",text)\n",
    "    text = re.sub(\"you’ll\",\"you will\",text)\n",
    "    text = re.sub(\"we’ll\",\"we will\",text)\n",
    "    text = re.sub(\"didn’t\",\"did not\",text)\n",
    "    text = re.sub(\"doesn’t\",\"does not\",text)\n",
    "    text = re.sub(\"aren’t\",\"are not\",text)\n",
    "    text = re.sub(\"don’t\",\"do not\",text)\n",
    "    text = re.sub(\"i’ve\",\"i have\",text)\n",
    "    text = re.sub(\"you’ve\",\"you have\",text)\n",
    "    text = re.sub(\"we’ve\",\"we have\",text)\n",
    "    text = re.sub(\"they’ve\",\"they have\",text)\n",
    "    text = re.sub(\"ain’t\",\"am not\",text)\n",
    "    text = re.sub(\"wouldn’t\",\"would not\",text)\n",
    "    text = re.sub(\"shouldn’t\",\"should not\",text)\n",
    "    text = re.sub(\"can’t\",\"can not\",text)\n",
    "    text = re.sub(\"couldn’t\",\"could not\",text)\n",
    "    text = re.sub(\"won’t\",\"will not\",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Removing Special Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    \n",
    "    '''Make text lowercase.'''\n",
    "    text = text.lower()\n",
    "    \n",
    "    '''Removing text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = re.sub('\\[.*?\\]', '', text) \n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text) \n",
    "    text = re.sub('\\w*\\d\\w*', '', text) \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n",
    "    \n",
    "    '''Removing extra spaces'''\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    text = re.sub(r'\\s+[a-z]\\s+',' ',text)\n",
    "    text = re.sub(r'^[a-z]\\s+',' ',text)\n",
    "\n",
    "    '''Get rid of some additional punctuation and non-sensical text.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "        \n",
    "    ''' Get rid of accented characters'''\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii','ignore').decode('utf-8','ignore')\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stemmer(text):\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    text = ' '.join([ps.stem(word) for word in text.split()])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_lemmatizer(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Removing Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_remover(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return filtered_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building The Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-128-aed10595b868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_cleaner\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_stemmer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_lemmatizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"clean_text\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords_remover\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[0;32m   4198\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4199\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4200\u001b[1;33m                 \u001b[0mmapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4201\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4202\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-126-1ab3a7e154c3>\u001b[0m in \u001b[0;36mtext_lemmatizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtext_lemmatizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlemma_\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'-PRON-'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "first_data[\"clean_text\"] = pd.DataFrame(first_data[\"transcript\"].apply(expand_contractions))\n",
    "first_data[\"clean_text\"] = pd.DataFrame(first_data[\"clean_text\"].apply(text_cleaner))\n",
    "first_data[\"clean_text\"] = pd.DataFrame(first_data[\"clean_text\"].apply(text_stemmer))\n",
    "first_data[\"clean_text\"] = pd.DataFrame(first_data[\"clean_text\"].apply(text_lemmatizer))\n",
    "first_data[\"clean_text\"] = pd.DataFrame(first_data[\"clean_text\"].apply(stopwords_remover))\n",
    "\n",
    "\n",
    "second_data[\"clean_text\"] = pd.DataFrame(second_data[\"transcript\"].apply(expand_contractions))\n",
    "second_data[\"clean_text\"] = pd.DataFrame(second_data[\"clean_text\"].apply(text_cleaner))\n",
    "second_data[\"clean_text\"] = pd.DataFrame(second_data[\"clean_text\"].apply(text_stemmer))\n",
    "second_data[\"clean_text\"] = pd.DataFrame(second_data[\"clean_text\"].apply(text_lemmatizer))\n",
    "second_data[\"clean_text\"] = pd.DataFrame(second_data[\"clean_text\"].apply(stopwords_remover))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
